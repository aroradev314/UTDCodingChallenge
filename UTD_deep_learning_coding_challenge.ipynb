{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ade97895",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import torchsummary\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7c266aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "619e54d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vehicle(Dataset):\n",
    "    def __init__(self, vehicles, labels):\n",
    "        self.vehicles = vehicles\n",
    "        self.labels = labels\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.vehicles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.vehicles[idx], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69527c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = os.getcwd()\n",
    "images_directory = os.path.join(root, \"vehicle_classification\")\n",
    "images = []\n",
    "labels = []\n",
    "current_label = 0\n",
    "for label in os.listdir(images_directory): #loop over every class\n",
    "    if not label.startswith('.'): # skip hidden directories\n",
    "        label_directory = os.path.join(images_directory, label)\n",
    "        for pic in os.listdir(label_directory): \n",
    "            image = Image.open(os.path.join(label_directory, pic))\n",
    "            image_array = np.array(image)\n",
    "            image_array = np.transpose(image_array, (2, 0, 1)) #make channels-first, as this is what PyTorch supports\n",
    "            images.append(image_array)\n",
    "            labels.append(current_label)\n",
    "        current_label += 1\n",
    "images = np.array(images)\n",
    "labels = np.array(labels)\n",
    "classes = np.unique(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1d28753",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 26378 images in the dataset\n",
      "Each image has dimensions 64x64\n",
      "The total number of classes is 8\n"
     ]
    }
   ],
   "source": [
    "num_classes = len(classes)\n",
    "image_height, image_length = len(images[0][0]), len(images[0][0][0])\n",
    "print(f\"There are {len(images)} images in the dataset\")\n",
    "print(f\"Each image has dimensions {image_length}x{image_height}\")\n",
    "print(f\"The total number of classes is {num_classes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a16434b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "images, labels, test_size=0.20, random_state=42) #split into random train and test sets\n",
    "X_train = torch.tensor(X_train).to(device)\n",
    "X_test = torch.tensor(X_test).to(device)\n",
    "y_train = torch.tensor(y_train).type(torch.LongTensor).to(device)\n",
    "y_test = torch.tensor(y_test).type(torch.LongTensor).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c99d3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "kernel_size = 3\n",
    "train_dataloader = DataLoader(Vehicle(X_train, y_train), batch_size=batch_size)\n",
    "test_dataloader = DataLoader(Vehicle(X_test, y_test), batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "172b3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        modules = []\n",
    "        modules.append(\n",
    "            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=kernel_size, padding='same')) \n",
    "        modules.append(\n",
    "            nn.Dropout(0.5)) # some inputs are randomly dropped\n",
    "        modules.append(\n",
    "            nn.ReLU())\n",
    "        modules.append(\n",
    "            nn.MaxPool2d(kernel_size=kernel_size * 2))\n",
    "        \n",
    "        modules.append(\n",
    "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=kernel_size, padding='same')) \n",
    "        modules.append(\n",
    "            nn.Dropout(0.5)) # some inputs are randomly dropped\n",
    "        modules.append(\n",
    "            nn.ReLU())\n",
    "        modules.append(\n",
    "            nn.MaxPool2d(kernel_size=kernel_size * 2)) \n",
    "        \n",
    "        modules.append(nn.Flatten())\n",
    "        modules.append(nn.Linear(16, 256)) # dense layer with 256 neurons \n",
    "        modules.append(nn.ReLU())\n",
    "        modules.append(\n",
    "            nn.Linear(256, num_classes))\n",
    "        modules.append(\n",
    "            nn.LogSoftmax(dim=1))\n",
    "    \n",
    "        self.network = nn.Sequential(*modules) # unpack the modules\n",
    "    \n",
    "    # Returns the summary of the model architecture given an input size\n",
    "    def summary(self): \n",
    "        return torchsummary.summary(self, input_size=(3, image_height, image_length))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        prob = self.network(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a2317d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    correct = 0\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X = X.type(torch.FloatTensor)\n",
    "        # Compute prediction error\n",
    "        pred = model(X).to(device)\n",
    "        loss = loss_fn(pred, y).to(device)\n",
    "        \n",
    "        _, predicted = torch.max(pred.data, 1)\n",
    "        correct += (predicted == y).sum().item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "    accuracy = correct / size\n",
    "    print(f\"Training accuracy: {accuracy:>2f}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb9bec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.type(torch.FloatTensor)\n",
    "            pred = model(X).to(device) # compute the model prediction\n",
    "            \n",
    "            # the predicted class is the one with the highest probability \n",
    "            # in the output distribution\n",
    "            _, predicted = torch.max(pred.data, 1)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "            test_loss += loss_fn(pred, y).to(device).item()\n",
    "            \n",
    "            \n",
    "    test_loss /= num_batches\n",
    "    accuracy = correct / (size)\n",
    "    print(f\"Avg loss: {test_loss:>8f}\")\n",
    "    print(f\"Testing accuracy: {accuracy:>2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "364f862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1           [-1, 16, 64, 64]             448\n",
      "           Dropout-2           [-1, 16, 64, 64]               0\n",
      "              ReLU-3           [-1, 16, 64, 64]               0\n",
      "         MaxPool2d-4           [-1, 16, 10, 10]               0\n",
      "            Conv2d-5           [-1, 16, 10, 10]           2,320\n",
      "           Dropout-6           [-1, 16, 10, 10]               0\n",
      "              ReLU-7           [-1, 16, 10, 10]               0\n",
      "         MaxPool2d-8             [-1, 16, 1, 1]               0\n",
      "           Flatten-9                   [-1, 16]               0\n",
      "           Linear-10                  [-1, 256]           4,352\n",
      "             ReLU-11                  [-1, 256]               0\n",
      "           Linear-12                    [-1, 8]           2,056\n",
      "       LogSoftmax-13                    [-1, 8]               0\n",
      "================================================================\n",
      "Total params: 9,176\n",
      "Trainable params: 9,176\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 1.55\n",
      "Params size (MB): 0.04\n",
      "Estimated Total Size (MB): 1.63\n",
      "----------------------------------------------------------------\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = CNN().to(device)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "20b61428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 37.268097  [    0/21102]\n",
      "loss: 1.523756  [ 6400/21102]\n",
      "loss: 1.736339  [12800/21102]\n",
      "loss: 1.416410  [19200/21102]\n",
      "Training accuracy: 0.485357\n",
      "Avg loss: 1.549337\n",
      "Testing accuracy: 0.582828\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.432497  [    0/21102]\n",
      "loss: 1.411723  [ 6400/21102]\n",
      "loss: 1.503060  [12800/21102]\n",
      "loss: 1.194555  [19200/21102]\n",
      "Training accuracy: 0.571178\n",
      "Avg loss: 1.390392\n",
      "Testing accuracy: 0.605951\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.362448  [    0/21102]\n",
      "loss: 1.494856  [ 6400/21102]\n",
      "loss: 1.035034  [12800/21102]\n",
      "loss: 1.105680  [19200/21102]\n",
      "Training accuracy: 0.596010\n",
      "Avg loss: 1.314736\n",
      "Testing accuracy: 0.605951\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 1.198830  [    0/21102]\n",
      "loss: 1.207952  [ 6400/21102]\n",
      "loss: 1.047988  [12800/21102]\n",
      "loss: 1.061783  [19200/21102]\n",
      "Training accuracy: 0.613070\n",
      "Avg loss: 1.228951\n",
      "Testing accuracy: 0.601213\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 1.130468  [    0/21102]\n",
      "loss: 1.123744  [ 6400/21102]\n",
      "loss: 0.989052  [12800/21102]\n",
      "loss: 0.940850  [19200/21102]\n",
      "Training accuracy: 0.624680\n",
      "Avg loss: 1.182691\n",
      "Testing accuracy: 0.604435\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 1.101552  [    0/21102]\n",
      "loss: 0.995623  [ 6400/21102]\n",
      "loss: 0.966709  [12800/21102]\n",
      "loss: 1.013961  [19200/21102]\n",
      "Training accuracy: 0.633494\n",
      "Avg loss: 1.139630\n",
      "Testing accuracy: 0.614481\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 1.061024  [    0/21102]\n",
      "loss: 1.081406  [ 6400/21102]\n",
      "loss: 0.928468  [12800/21102]\n",
      "loss: 0.851583  [19200/21102]\n",
      "Training accuracy: 0.644299\n",
      "Avg loss: 1.127874\n",
      "Testing accuracy: 0.613154\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 1.009141  [    0/21102]\n",
      "loss: 0.927942  [ 6400/21102]\n",
      "loss: 0.827065  [12800/21102]\n",
      "loss: 0.855799  [19200/21102]\n",
      "Training accuracy: 0.646053\n",
      "Avg loss: 1.115316\n",
      "Testing accuracy: 0.615239\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 1.015979  [    0/21102]\n",
      "loss: 1.055028  [ 6400/21102]\n",
      "loss: 0.788074  [12800/21102]\n",
      "loss: 0.730176  [19200/21102]\n",
      "Training accuracy: 0.653066\n",
      "Avg loss: 1.148216\n",
      "Testing accuracy: 0.594011\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.949828  [    0/21102]\n",
      "loss: 0.911356  [ 6400/21102]\n",
      "loss: 0.922996  [12800/21102]\n",
      "loss: 0.776781  [19200/21102]\n",
      "Training accuracy: 0.655530\n",
      "Avg loss: 1.131873\n",
      "Testing accuracy: 0.606520\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.869435  [    0/21102]\n",
      "loss: 0.897974  [ 6400/21102]\n",
      "loss: 0.868993  [12800/21102]\n",
      "loss: 0.788342  [19200/21102]\n",
      "Training accuracy: 0.657236\n",
      "Avg loss: 1.098489\n",
      "Testing accuracy: 0.618840\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.843919  [    0/21102]\n",
      "loss: 0.965826  [ 6400/21102]\n",
      "loss: 0.814391  [12800/21102]\n",
      "loss: 0.782633  [19200/21102]\n",
      "Training accuracy: 0.661264\n",
      "Avg loss: 1.147927\n",
      "Testing accuracy: 0.587187\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.999503  [    0/21102]\n",
      "loss: 0.995602  [ 6400/21102]\n",
      "loss: 0.787872  [12800/21102]\n",
      "loss: 0.866584  [19200/21102]\n",
      "Training accuracy: 0.662544\n",
      "Avg loss: 1.219996\n",
      "Testing accuracy: 0.574867\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.819614  [    0/21102]\n",
      "loss: 0.787441  [ 6400/21102]\n",
      "loss: 0.736908  [12800/21102]\n",
      "loss: 0.757579  [19200/21102]\n",
      "Training accuracy: 0.673064\n",
      "Avg loss: 1.165418\n",
      "Testing accuracy: 0.589651\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.947420  [    0/21102]\n",
      "loss: 0.772473  [ 6400/21102]\n",
      "loss: 0.761453  [12800/21102]\n",
      "loss: 0.782336  [19200/21102]\n",
      "Training accuracy: 0.669747\n",
      "Avg loss: 1.135909\n",
      "Testing accuracy: 0.602729\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.931206  [    0/21102]\n",
      "loss: 0.766550  [ 6400/21102]\n",
      "loss: 0.777455  [12800/21102]\n",
      "loss: 0.716018  [19200/21102]\n",
      "Training accuracy: 0.679177\n",
      "Avg loss: 1.142310\n",
      "Testing accuracy: 0.598180\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.861101  [    0/21102]\n",
      "loss: 0.823374  [ 6400/21102]\n",
      "loss: 0.772084  [12800/21102]\n",
      "loss: 0.698676  [19200/21102]\n",
      "Training accuracy: 0.676524\n",
      "Avg loss: 1.119039\n",
      "Testing accuracy: 0.615239\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.834304  [    0/21102]\n",
      "loss: 0.725053  [ 6400/21102]\n",
      "loss: 0.803981  [12800/21102]\n",
      "loss: 0.710226  [19200/21102]\n",
      "Training accuracy: 0.679130\n",
      "Avg loss: 1.129497\n",
      "Testing accuracy: 0.607847\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.846579  [    0/21102]\n",
      "loss: 0.766880  [ 6400/21102]\n",
      "loss: 0.884936  [12800/21102]\n",
      "loss: 0.723358  [19200/21102]\n",
      "Training accuracy: 0.678561\n",
      "Avg loss: 1.182174\n",
      "Testing accuracy: 0.588893\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.872306  [    0/21102]\n",
      "loss: 0.785680  [ 6400/21102]\n",
      "loss: 0.781006  [12800/21102]\n",
      "loss: 0.852606  [19200/21102]\n",
      "Training accuracy: 0.683774\n",
      "Avg loss: 1.124084\n",
      "Testing accuracy: 0.617324\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.793098  [    0/21102]\n",
      "loss: 0.825299  [ 6400/21102]\n",
      "loss: 0.805048  [12800/21102]\n",
      "loss: 0.804711  [19200/21102]\n",
      "Training accuracy: 0.684627\n",
      "Avg loss: 1.192356\n",
      "Testing accuracy: 0.593252\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.827198  [    0/21102]\n",
      "loss: 0.745515  [ 6400/21102]\n",
      "loss: 0.807927  [12800/21102]\n",
      "loss: 0.773884  [19200/21102]\n",
      "Training accuracy: 0.686665\n",
      "Avg loss: 1.133234\n",
      "Testing accuracy: 0.599318\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.858744  [    0/21102]\n",
      "loss: 0.763683  [ 6400/21102]\n",
      "loss: 0.709887  [12800/21102]\n",
      "loss: 0.707557  [19200/21102]\n",
      "Training accuracy: 0.689840\n",
      "Avg loss: 1.136561\n",
      "Testing accuracy: 0.611638\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.850675  [    0/21102]\n",
      "loss: 0.729249  [ 6400/21102]\n",
      "loss: 0.763105  [12800/21102]\n",
      "loss: 0.696473  [19200/21102]\n",
      "Training accuracy: 0.695763\n",
      "Avg loss: 1.179129\n",
      "Testing accuracy: 0.604435\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.872344  [    0/21102]\n",
      "loss: 0.703238  [ 6400/21102]\n",
      "loss: 0.680594  [12800/21102]\n",
      "loss: 0.695087  [19200/21102]\n",
      "Training accuracy: 0.693915\n",
      "Avg loss: 1.132905\n",
      "Testing accuracy: 0.613723\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.724733  [    0/21102]\n",
      "loss: 0.658003  [ 6400/21102]\n",
      "loss: 0.785475  [12800/21102]\n",
      "loss: 0.646355  [19200/21102]\n",
      "Training accuracy: 0.695337\n",
      "Avg loss: 1.082459\n",
      "Testing accuracy: 0.628696\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.772462  [    0/21102]\n",
      "loss: 0.720011  [ 6400/21102]\n",
      "loss: 0.759319  [12800/21102]\n",
      "loss: 0.665055  [19200/21102]\n",
      "Training accuracy: 0.700265\n",
      "Avg loss: 1.180993\n",
      "Testing accuracy: 0.612964\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.892630  [    0/21102]\n",
      "loss: 0.635919  [ 6400/21102]\n",
      "loss: 0.825306  [12800/21102]\n",
      "loss: 0.681235  [19200/21102]\n",
      "Training accuracy: 0.704151\n",
      "Avg loss: 1.110248\n",
      "Testing accuracy: 0.619788\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.801972  [    0/21102]\n",
      "loss: 0.679104  [ 6400/21102]\n",
      "loss: 0.764010  [12800/21102]\n",
      "loss: 0.705625  [19200/21102]\n",
      "Training accuracy: 0.701545\n",
      "Avg loss: 1.098491\n",
      "Testing accuracy: 0.615239\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.771595  [    0/21102]\n",
      "loss: 0.675190  [ 6400/21102]\n",
      "loss: 0.744721  [12800/21102]\n",
      "loss: 0.650431  [19200/21102]\n",
      "Training accuracy: 0.702730\n",
      "Avg loss: 1.113783\n",
      "Testing accuracy: 0.623768\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.850398  [    0/21102]\n",
      "loss: 0.665619  [ 6400/21102]\n",
      "loss: 0.744052  [12800/21102]\n",
      "loss: 0.582430  [19200/21102]\n",
      "Training accuracy: 0.706094\n",
      "Avg loss: 1.161004\n",
      "Testing accuracy: 0.614860\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.851680  [    0/21102]\n",
      "loss: 0.614814  [ 6400/21102]\n",
      "loss: 0.712914  [12800/21102]\n",
      "loss: 0.657807  [19200/21102]\n",
      "Training accuracy: 0.709554\n",
      "Avg loss: 1.148467\n",
      "Testing accuracy: 0.599128\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.806727  [    0/21102]\n",
      "loss: 0.643740  [ 6400/21102]\n",
      "loss: 0.679902  [12800/21102]\n",
      "loss: 0.622330  [19200/21102]\n",
      "Training accuracy: 0.709554\n",
      "Avg loss: 1.082222\n",
      "Testing accuracy: 0.628317\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.831559  [    0/21102]\n",
      "loss: 0.564264  [ 6400/21102]\n",
      "loss: 0.713212  [12800/21102]\n",
      "loss: 0.640669  [19200/21102]\n",
      "Training accuracy: 0.706900\n",
      "Avg loss: 1.065587\n",
      "Testing accuracy: 0.626422\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.813619  [    0/21102]\n",
      "loss: 0.535421  [ 6400/21102]\n",
      "loss: 0.698636  [12800/21102]\n",
      "loss: 0.668300  [19200/21102]\n",
      "Training accuracy: 0.709554\n",
      "Avg loss: 1.100176\n",
      "Testing accuracy: 0.620546\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.790071  [    0/21102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.607335  [ 6400/21102]\n",
      "loss: 0.740734  [12800/21102]\n",
      "loss: 0.667762  [19200/21102]\n",
      "Training accuracy: 0.711260\n",
      "Avg loss: 1.128214\n",
      "Testing accuracy: 0.619598\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.760747  [    0/21102]\n",
      "loss: 0.672378  [ 6400/21102]\n",
      "loss: 0.767558  [12800/21102]\n",
      "loss: 0.619254  [19200/21102]\n",
      "Training accuracy: 0.710691\n",
      "Avg loss: 1.037144\n",
      "Testing accuracy: 0.641016\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.812468  [    0/21102]\n",
      "loss: 0.613409  [ 6400/21102]\n",
      "loss: 0.652341  [12800/21102]\n",
      "loss: 0.651745  [19200/21102]\n",
      "Training accuracy: 0.710312\n",
      "Avg loss: 1.135476\n",
      "Testing accuracy: 0.619598\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.787280  [    0/21102]\n",
      "loss: 0.695933  [ 6400/21102]\n",
      "loss: 0.763229  [12800/21102]\n",
      "loss: 0.638254  [19200/21102]\n",
      "Training accuracy: 0.711876\n",
      "Avg loss: 1.026963\n",
      "Testing accuracy: 0.635330\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.796341  [    0/21102]\n",
      "loss: 0.589584  [ 6400/21102]\n",
      "loss: 0.728975  [12800/21102]\n",
      "loss: 0.622040  [19200/21102]\n",
      "Training accuracy: 0.713771\n",
      "Avg loss: 1.090631\n",
      "Testing accuracy: 0.623578\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.759108  [    0/21102]\n",
      "loss: 0.562597  [ 6400/21102]\n",
      "loss: 0.859152  [12800/21102]\n",
      "loss: 0.657690  [19200/21102]\n",
      "Training accuracy: 0.711923\n",
      "Avg loss: 1.064801\n",
      "Testing accuracy: 0.634382\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.712987  [    0/21102]\n",
      "loss: 0.585963  [ 6400/21102]\n",
      "loss: 0.713337  [12800/21102]\n",
      "loss: 0.642942  [19200/21102]\n",
      "Training accuracy: 0.712492\n",
      "Avg loss: 1.017124\n",
      "Testing accuracy: 0.656748\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.711123  [    0/21102]\n",
      "loss: 0.699775  [ 6400/21102]\n",
      "loss: 0.764323  [12800/21102]\n",
      "loss: 0.643380  [19200/21102]\n",
      "Training accuracy: 0.709885\n",
      "Avg loss: 1.034636\n",
      "Testing accuracy: 0.633245\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.778896  [    0/21102]\n",
      "loss: 0.599479  [ 6400/21102]\n",
      "loss: 0.750204  [12800/21102]\n",
      "loss: 0.740131  [19200/21102]\n",
      "Training accuracy: 0.713534\n",
      "Avg loss: 1.056601\n",
      "Testing accuracy: 0.634193\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.783512  [    0/21102]\n",
      "loss: 0.675601  [ 6400/21102]\n",
      "loss: 0.721828  [12800/21102]\n",
      "loss: 0.745781  [19200/21102]\n",
      "Training accuracy: 0.714198\n",
      "Avg loss: 1.058733\n",
      "Testing accuracy: 0.623389\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.806020  [    0/21102]\n",
      "loss: 0.667946  [ 6400/21102]\n",
      "loss: 0.777222  [12800/21102]\n",
      "loss: 0.696057  [19200/21102]\n",
      "Training accuracy: 0.717799\n",
      "Avg loss: 1.002144\n",
      "Testing accuracy: 0.655800\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.783679  [    0/21102]\n",
      "loss: 0.715865  [ 6400/21102]\n",
      "loss: 0.724042  [12800/21102]\n",
      "loss: 0.649667  [19200/21102]\n",
      "Training accuracy: 0.714672\n",
      "Avg loss: 1.053449\n",
      "Testing accuracy: 0.644617\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.770965  [    0/21102]\n",
      "loss: 0.652359  [ 6400/21102]\n",
      "loss: 0.686278  [12800/21102]\n",
      "loss: 0.627577  [19200/21102]\n",
      "Training accuracy: 0.714529\n",
      "Avg loss: 1.070134\n",
      "Testing accuracy: 0.626042\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.758661  [    0/21102]\n",
      "loss: 0.681433  [ 6400/21102]\n",
      "loss: 0.727673  [12800/21102]\n",
      "loss: 0.652779  [19200/21102]\n",
      "Training accuracy: 0.715193\n",
      "Avg loss: 0.998911\n",
      "Testing accuracy: 0.654663\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.738499  [    0/21102]\n",
      "loss: 0.563952  [ 6400/21102]\n",
      "loss: 0.823146  [12800/21102]\n",
      "loss: 0.624625  [19200/21102]\n",
      "Training accuracy: 0.715003\n",
      "Avg loss: 1.024329\n",
      "Testing accuracy: 0.656748\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.706631  [    0/21102]\n",
      "loss: 0.774607  [ 6400/21102]\n",
      "loss: 0.759823  [12800/21102]\n",
      "loss: 0.595236  [19200/21102]\n",
      "Training accuracy: 0.714909\n",
      "Avg loss: 1.042104\n",
      "Testing accuracy: 0.647460\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.731377  [    0/21102]\n",
      "loss: 0.575236  [ 6400/21102]\n",
      "loss: 0.709072  [12800/21102]\n",
      "loss: 0.577796  [19200/21102]\n",
      "Training accuracy: 0.715714\n",
      "Avg loss: 1.042597\n",
      "Testing accuracy: 0.643859\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.809347  [    0/21102]\n",
      "loss: 0.611920  [ 6400/21102]\n",
      "loss: 0.792224  [12800/21102]\n",
      "loss: 0.614205  [19200/21102]\n",
      "Training accuracy: 0.714198\n",
      "Avg loss: 1.033038\n",
      "Testing accuracy: 0.644428\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.758700  [    0/21102]\n",
      "loss: 0.720488  [ 6400/21102]\n",
      "loss: 0.776127  [12800/21102]\n",
      "loss: 0.552237  [19200/21102]\n",
      "Training accuracy: 0.716946\n",
      "Avg loss: 0.976258\n",
      "Testing accuracy: 0.658643\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.823040  [    0/21102]\n",
      "loss: 0.640782  [ 6400/21102]\n",
      "loss: 0.703175  [12800/21102]\n",
      "loss: 0.582698  [19200/21102]\n",
      "Training accuracy: 0.720595\n",
      "Avg loss: 1.052083\n",
      "Testing accuracy: 0.638362\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.749883  [    0/21102]\n",
      "loss: 0.616614  [ 6400/21102]\n",
      "loss: 0.731104  [12800/21102]\n",
      "loss: 0.619106  [19200/21102]\n",
      "Training accuracy: 0.721022\n",
      "Avg loss: 1.026159\n",
      "Testing accuracy: 0.649924\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.754624  [    0/21102]\n",
      "loss: 0.631317  [ 6400/21102]\n",
      "loss: 0.813364  [12800/21102]\n",
      "loss: 0.541896  [19200/21102]\n",
      "Training accuracy: 0.717183\n",
      "Avg loss: 1.028770\n",
      "Testing accuracy: 0.646513\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.733533  [    0/21102]\n",
      "loss: 0.536986  [ 6400/21102]\n",
      "loss: 0.750813  [12800/21102]\n",
      "loss: 0.626966  [19200/21102]\n",
      "Training accuracy: 0.719979\n",
      "Avg loss: 1.005396\n",
      "Testing accuracy: 0.659591\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.727966  [    0/21102]\n",
      "loss: 0.643396  [ 6400/21102]\n",
      "loss: 0.731962  [12800/21102]\n",
      "loss: 0.610198  [19200/21102]\n",
      "Training accuracy: 0.717704\n",
      "Avg loss: 0.991402\n",
      "Testing accuracy: 0.665656\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.826335  [    0/21102]\n",
      "loss: 0.636213  [ 6400/21102]\n",
      "loss: 0.843254  [12800/21102]\n",
      "loss: 0.610805  [19200/21102]\n",
      "Training accuracy: 0.717325\n",
      "Avg loss: 1.031675\n",
      "Testing accuracy: 0.651440\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.755422  [    0/21102]\n",
      "loss: 0.507330  [ 6400/21102]\n",
      "loss: 0.960894  [12800/21102]\n",
      "loss: 0.637607  [19200/21102]\n",
      "Training accuracy: 0.717278\n",
      "Avg loss: 1.058955\n",
      "Testing accuracy: 0.641774\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.815348  [    0/21102]\n",
      "loss: 0.534271  [ 6400/21102]\n",
      "loss: 0.812154  [12800/21102]\n",
      "loss: 0.576347  [19200/21102]\n",
      "Training accuracy: 0.719126\n",
      "Avg loss: 1.038961\n",
      "Testing accuracy: 0.662055\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.737717  [    0/21102]\n",
      "loss: 0.553487  [ 6400/21102]\n",
      "loss: 0.802373  [12800/21102]\n",
      "loss: 0.590741  [19200/21102]\n",
      "Training accuracy: 0.718984\n",
      "Avg loss: 1.012939\n",
      "Testing accuracy: 0.657316\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.701653  [    0/21102]\n",
      "loss: 0.605398  [ 6400/21102]\n",
      "loss: 0.768658  [12800/21102]\n",
      "loss: 0.620432  [19200/21102]\n",
      "Training accuracy: 0.717278\n",
      "Avg loss: 1.030698\n",
      "Testing accuracy: 0.644807\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.713913  [    0/21102]\n",
      "loss: 0.715453  [ 6400/21102]\n",
      "loss: 0.740832  [12800/21102]\n",
      "loss: 0.521725  [19200/21102]\n",
      "Training accuracy: 0.721638\n",
      "Avg loss: 1.033319\n",
      "Testing accuracy: 0.643290\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.760752  [    0/21102]\n",
      "loss: 0.668405  [ 6400/21102]\n",
      "loss: 0.703049  [12800/21102]\n",
      "loss: 0.603474  [19200/21102]\n",
      "Training accuracy: 0.725571\n",
      "Avg loss: 1.006403\n",
      "Testing accuracy: 0.657695\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.742743  [    0/21102]\n",
      "loss: 0.606113  [ 6400/21102]\n",
      "loss: 0.749626  [12800/21102]\n",
      "loss: 0.585066  [19200/21102]\n",
      "Training accuracy: 0.721922\n",
      "Avg loss: 1.033492\n",
      "Testing accuracy: 0.648597\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.769000  [    0/21102]\n",
      "loss: 0.627849  [ 6400/21102]\n",
      "loss: 0.655429  [12800/21102]\n",
      "loss: 0.590771  [19200/21102]\n",
      "Training accuracy: 0.724434\n",
      "Avg loss: 0.986486\n",
      "Testing accuracy: 0.656558\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.746317  [    0/21102]\n",
      "loss: 0.631357  [ 6400/21102]\n",
      "loss: 0.712895  [12800/21102]\n",
      "loss: 0.553050  [19200/21102]\n",
      "Training accuracy: 0.721969\n",
      "Avg loss: 0.998637\n",
      "Testing accuracy: 0.653715\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.745824  [    0/21102]\n",
      "loss: 0.577336  [ 6400/21102]\n",
      "loss: 0.757045  [12800/21102]\n",
      "loss: 0.570990  [19200/21102]\n",
      "Training accuracy: 0.721496\n",
      "Avg loss: 0.977828\n",
      "Testing accuracy: 0.666414\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.706561  [    0/21102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.612040  [ 6400/21102]\n",
      "loss: 0.777130  [12800/21102]\n",
      "loss: 0.552812  [19200/21102]\n",
      "Training accuracy: 0.721543\n",
      "Avg loss: 0.995419\n",
      "Testing accuracy: 0.663002\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.763955  [    0/21102]\n",
      "loss: 0.625608  [ 6400/21102]\n",
      "loss: 0.695350  [12800/21102]\n",
      "loss: 0.591969  [19200/21102]\n",
      "Training accuracy: 0.722822\n",
      "Avg loss: 1.036278\n",
      "Testing accuracy: 0.654663\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.732814  [    0/21102]\n",
      "loss: 0.590491  [ 6400/21102]\n",
      "loss: 0.802977  [12800/21102]\n",
      "loss: 0.539441  [19200/21102]\n",
      "Training accuracy: 0.725998\n",
      "Avg loss: 1.026806\n",
      "Testing accuracy: 0.653715\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.760738  [    0/21102]\n",
      "loss: 0.651336  [ 6400/21102]\n",
      "loss: 0.823899  [12800/21102]\n",
      "loss: 0.528033  [19200/21102]\n",
      "Training accuracy: 0.717941\n",
      "Avg loss: 1.004054\n",
      "Testing accuracy: 0.656558\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.799690  [    0/21102]\n",
      "loss: 0.695331  [ 6400/21102]\n",
      "loss: 0.753501  [12800/21102]\n",
      "loss: 0.525284  [19200/21102]\n",
      "Training accuracy: 0.722443\n",
      "Avg loss: 1.040558\n",
      "Testing accuracy: 0.647271\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.673195  [    0/21102]\n",
      "loss: 0.680657  [ 6400/21102]\n",
      "loss: 0.786470  [12800/21102]\n",
      "loss: 0.513934  [19200/21102]\n",
      "Training accuracy: 0.725050\n",
      "Avg loss: 1.004254\n",
      "Testing accuracy: 0.658074\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.778736  [    0/21102]\n",
      "loss: 0.528687  [ 6400/21102]\n",
      "loss: 0.722120  [12800/21102]\n",
      "loss: 0.629201  [19200/21102]\n",
      "Training accuracy: 0.725808\n",
      "Avg loss: 1.024525\n",
      "Testing accuracy: 0.648408\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.712716  [    0/21102]\n",
      "loss: 0.677067  [ 6400/21102]\n",
      "loss: 0.781173  [12800/21102]\n",
      "loss: 0.644833  [19200/21102]\n",
      "Training accuracy: 0.724623\n",
      "Avg loss: 0.997527\n",
      "Testing accuracy: 0.665466\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.766138  [    0/21102]\n",
      "loss: 0.652902  [ 6400/21102]\n",
      "loss: 0.740669  [12800/21102]\n",
      "loss: 0.531893  [19200/21102]\n",
      "Training accuracy: 0.726519\n",
      "Avg loss: 1.019131\n",
      "Testing accuracy: 0.659212\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.706282  [    0/21102]\n",
      "loss: 0.709014  [ 6400/21102]\n",
      "loss: 0.743512  [12800/21102]\n",
      "loss: 0.588021  [19200/21102]\n",
      "Training accuracy: 0.724292\n",
      "Avg loss: 1.043445\n",
      "Testing accuracy: 0.639689\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.766872  [    0/21102]\n",
      "loss: 0.573267  [ 6400/21102]\n",
      "loss: 0.805334  [12800/21102]\n",
      "loss: 0.553269  [19200/21102]\n",
      "Training accuracy: 0.722491\n",
      "Avg loss: 1.023413\n",
      "Testing accuracy: 0.654852\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.671022  [    0/21102]\n",
      "loss: 0.596863  [ 6400/21102]\n",
      "loss: 0.802736  [12800/21102]\n",
      "loss: 0.560665  [19200/21102]\n",
      "Training accuracy: 0.726234\n",
      "Avg loss: 1.066931\n",
      "Testing accuracy: 0.643101\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.700161  [    0/21102]\n",
      "loss: 0.549134  [ 6400/21102]\n",
      "loss: 0.845441  [12800/21102]\n",
      "loss: 0.555522  [19200/21102]\n",
      "Training accuracy: 0.723344\n",
      "Avg loss: 1.107623\n",
      "Testing accuracy: 0.628317\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.749739  [    0/21102]\n",
      "loss: 0.674448  [ 6400/21102]\n",
      "loss: 0.720225  [12800/21102]\n",
      "loss: 0.652696  [19200/21102]\n",
      "Training accuracy: 0.723344\n",
      "Avg loss: 1.015517\n",
      "Testing accuracy: 0.662434\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.715390  [    0/21102]\n",
      "loss: 0.679893  [ 6400/21102]\n",
      "loss: 0.822997  [12800/21102]\n",
      "loss: 0.572741  [19200/21102]\n",
      "Training accuracy: 0.727324\n",
      "Avg loss: 0.999068\n",
      "Testing accuracy: 0.656558\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.715387  [    0/21102]\n",
      "loss: 0.639250  [ 6400/21102]\n",
      "loss: 0.729916  [12800/21102]\n",
      "loss: 0.571146  [19200/21102]\n",
      "Training accuracy: 0.727372\n",
      "Avg loss: 1.004519\n",
      "Testing accuracy: 0.654094\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.659855  [    0/21102]\n",
      "loss: 0.491796  [ 6400/21102]\n",
      "loss: 0.754900  [12800/21102]\n",
      "loss: 0.634383  [19200/21102]\n",
      "Training accuracy: 0.725381\n",
      "Avg loss: 1.062272\n",
      "Testing accuracy: 0.649356\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.770741  [    0/21102]\n",
      "loss: 0.577861  [ 6400/21102]\n",
      "loss: 0.703253  [12800/21102]\n",
      "loss: 0.674507  [19200/21102]\n",
      "Training accuracy: 0.725855\n",
      "Avg loss: 1.028646\n",
      "Testing accuracy: 0.648976\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.875392  [    0/21102]\n",
      "loss: 0.613326  [ 6400/21102]\n",
      "loss: 0.796378  [12800/21102]\n",
      "loss: 0.607405  [19200/21102]\n",
      "Training accuracy: 0.726187\n",
      "Avg loss: 1.002174\n",
      "Testing accuracy: 0.660349\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.740259  [    0/21102]\n",
      "loss: 0.602238  [ 6400/21102]\n",
      "loss: 0.663037  [12800/21102]\n",
      "loss: 0.503165  [19200/21102]\n",
      "Training accuracy: 0.724149\n",
      "Avg loss: 1.003666\n",
      "Testing accuracy: 0.647650\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.800775  [    0/21102]\n",
      "loss: 0.676932  [ 6400/21102]\n",
      "loss: 0.727021  [12800/21102]\n",
      "loss: 0.515408  [19200/21102]\n",
      "Training accuracy: 0.727514\n",
      "Avg loss: 1.012111\n",
      "Testing accuracy: 0.664898\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.735959  [    0/21102]\n",
      "loss: 0.587260  [ 6400/21102]\n",
      "loss: 0.737280  [12800/21102]\n",
      "loss: 0.621747  [19200/21102]\n",
      "Training accuracy: 0.726471\n",
      "Avg loss: 1.025035\n",
      "Testing accuracy: 0.653525\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.750938  [    0/21102]\n",
      "loss: 0.583478  [ 6400/21102]\n",
      "loss: 0.746464  [12800/21102]\n",
      "loss: 0.553383  [19200/21102]\n",
      "Training accuracy: 0.727135\n",
      "Avg loss: 1.025237\n",
      "Testing accuracy: 0.661296\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.728866  [    0/21102]\n",
      "loss: 0.629788  [ 6400/21102]\n",
      "loss: 0.708930  [12800/21102]\n",
      "loss: 0.539056  [19200/21102]\n",
      "Training accuracy: 0.727751\n",
      "Avg loss: 1.003990\n",
      "Testing accuracy: 0.661865\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.750547  [    0/21102]\n",
      "loss: 0.594699  [ 6400/21102]\n",
      "loss: 0.753607  [12800/21102]\n",
      "loss: 0.540321  [19200/21102]\n",
      "Training accuracy: 0.727230\n",
      "Avg loss: 0.987619\n",
      "Testing accuracy: 0.671911\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.727274  [    0/21102]\n",
      "loss: 0.580948  [ 6400/21102]\n",
      "loss: 0.745767  [12800/21102]\n",
      "loss: 0.630812  [19200/21102]\n",
      "Training accuracy: 0.726993\n",
      "Avg loss: 1.049565\n",
      "Testing accuracy: 0.653904\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.753023  [    0/21102]\n",
      "loss: 0.599122  [ 6400/21102]\n",
      "loss: 0.694314  [12800/21102]\n",
      "loss: 0.579661  [19200/21102]\n",
      "Training accuracy: 0.728746\n",
      "Avg loss: 1.062261\n",
      "Testing accuracy: 0.648787\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.840553  [    0/21102]\n",
      "loss: 0.611814  [ 6400/21102]\n",
      "loss: 0.745359  [12800/21102]\n",
      "loss: 0.589581  [19200/21102]\n",
      "Training accuracy: 0.731874\n",
      "Avg loss: 1.013332\n",
      "Testing accuracy: 0.658264\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.751020  [    0/21102]\n",
      "loss: 0.575230  [ 6400/21102]\n",
      "loss: 0.785236  [12800/21102]\n",
      "loss: 0.621362  [19200/21102]\n",
      "Training accuracy: 0.730120\n",
      "Avg loss: 0.994242\n",
      "Testing accuracy: 0.664898\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.795525  [    0/21102]\n",
      "loss: 0.523706  [ 6400/21102]\n",
      "loss: 0.823381  [12800/21102]\n",
      "loss: 0.560402  [19200/21102]\n",
      "Training accuracy: 0.726661\n",
      "Avg loss: 1.023380\n",
      "Testing accuracy: 0.647839\n",
      "Done!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 2000x300 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "loss_fn = nn.NLLLoss().to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "plt.figure(figsize=(20, 3))\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c7f686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Vehicle Classifier",
   "language": "python",
   "name": "vehicle_classifier_model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
